{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hm3/Desktop/YT-content/transcript/.venv/lib/python3.12/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import whisper\n",
    "import torch\n",
    "import subprocess\n",
    "\n",
    " \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    " \n",
    "model = whisper.load_model(\"base\", device=device)\n",
    "\n",
    "def download_audio(youtube_url, output_path=\"audio.mp3\"):\n",
    "    \"\"\"Download the audio from a YouTube video.\"\"\"\n",
    "    try:\n",
    "        command = [\n",
    "            \"yt-dlp\",\n",
    "            \"--format\", \"bestaudio\",\n",
    "            \"--extract-audio\",\n",
    "            \"--audio-format\", \"mp3\",\n",
    "            \"--output\", output_path,\n",
    "            youtube_url,\n",
    "        ]\n",
    "        subprocess.run(command, check=True)\n",
    "        print(f\"Audio downloaded to {output_path}\")\n",
    "        return output_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading audio: {e}\")\n",
    "        return None\n",
    "\n",
    "def transcribe_audio(audio_path):\n",
    "    \"\"\"Transcribe the audio using Whisper.\"\"\"\n",
    "    try:\n",
    "        print(\"Extracting transcript\")\n",
    "        result = model.transcribe(audio_path)\n",
    "        return result  # Contains text and timestamps\n",
    "    except Exception as e:\n",
    "        print(f\"Error transcribing audio: {e}\")\n",
    "        return None\n",
    "\n",
    "def save_transcript_with_timestamps(transcript, output_file=\"transcript.txt\"):\n",
    "    \"\"\"Save the transcript with timestamps to a file.\"\"\"\n",
    "    try:\n",
    "        with open(output_file, \"w\") as file:\n",
    "            for segment in transcript['segments']:\n",
    "                start_time = segment['start']\n",
    "                end_time = segment['end']\n",
    "                text = segment['text']\n",
    "                file.write(f\"[{start_time:.2f} - {end_time:.2f}] {text}\\n\")\n",
    "        print(f\"Transcript saved to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving transcript: {e}\")\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "def detect_terms_to_exclude(english_text):\n",
    "    \"\"\"Detect terms that are likely to be technical or domain-specific.\"\"\"\n",
    "  \n",
    "    doc = nlp(english_text)\n",
    " \n",
    "    exclude_terms = []\n",
    " \n",
    "    for ent in doc.ents:\n",
    "        exclude_terms.append(ent.text)\n",
    "    \n",
    " \n",
    "    for token in doc:\n",
    "        if token.pos_ == \"PROPN\" or (token.is_upper and len(token) > 1):  \n",
    "            exclude_terms.append(token.text)\n",
    "\n",
    "    exclude_terms = [term for term in exclude_terms if not re.search(r\"(\\n|%|\\d{1,2}%|^\\d{1,2}$)\", term)]\n",
    "    exclude_terms = list(set(exclude_terms))\n",
    "    \n",
    "    return exclude_terms\n",
    "\n",
    " \n",
    " \n",
    "def generate_translation_prompt(english_text, exclude_terms, custom_terms):\n",
    " \n",
    "    exclusion_list = \"\\n\".join([f\"- {term}\" for term in exclude_terms])\n",
    " \n",
    "    custom_translations = \"\\n\".join([f\"- \\\"{key}\\\" should be translated as \\\"{value}\\\"\" for key, value in custom_terms.items()])\n",
    "    \n",
    "  \n",
    "    instructions = f\"\"\"\n",
    "    Translate the following English text to Burmese (Myanmar) while ensuring the following:\n",
    "    \n",
    "    1. **Exclude specific terms from translation**:\n",
    "        {exclusion_list}\n",
    "        These terms should remain in English and not be translated.\n",
    "    \n",
    "    2. **For specific phrases, use custom translations**:\n",
    "        {custom_translations}\n",
    "        These phrases should be translated exactly as indicated.\n",
    "\n",
    "    The translation should respect the context and accurately convey the meaning of the original text while maintaining these exclusions and custom translations.\n",
    "    \"\"\"\n",
    " \n",
    "    prompt = f\"{instructions}\\n\\nHere is the English text you need to translate:\\n\\n{english_text}\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def save_file(filepath: str, content: str):\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "        \n",
    "def read_file(filepath:str):\n",
    "    with open(filepath , \"r\") as f:\n",
    "        return f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hm3/Desktop/YT-content/transcript/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "import cohere\n",
    "\n",
    "def translate_english_to_myanmar_with_transformer(text, exclude_terms=None, custom_terms=None):\n",
    "    if exclude_terms is None:\n",
    "        exclude_terms = []\n",
    "    if custom_terms is None:\n",
    "        custom_terms = {}\n",
    " \n",
    "    placeholders = {}\n",
    "    for idx, (eng_term, myanmar_term) in enumerate(custom_terms.items()):\n",
    "        placeholder = f\"__CUSTOM_TERM_{idx}__\"\n",
    "        text = text.replace(eng_term, placeholder)\n",
    "        placeholders[placeholder] = myanmar_term\n",
    "\n",
    "    \n",
    "    for idx, term in enumerate(exclude_terms):\n",
    "        placeholder = f\"__TERM_{idx}__\"\n",
    "        text = text.replace(term, placeholder)\n",
    "        placeholders[placeholder] = term\n",
    "    print(f\"optimzed text: {text}\")\n",
    "    model_name = \"facebook/m2m100_418M\"\n",
    "    tokenizer = M2M100Tokenizer.from_pretrained(model_name)\n",
    "    model = M2M100ForConditionalGeneration.from_pretrained(model_name)\n",
    "    \n",
    "    tokenizer.src_lang = \"en\"\n",
    "    encoded_text = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "    \n",
    "    generated_tokens = model.generate(**encoded_text, forced_bos_token_id=tokenizer.get_lang_id(\"my\"))\n",
    "    translated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "     \n",
    "    for placeholder, term in placeholders.items():\n",
    "        translated_text = translated_text.replace(placeholder, term)\n",
    "\n",
    "    return translated_text\n",
    " \n",
    "api_key=\"\"\n",
    "co = cohere.Client(api_key=api_key)\n",
    "\n",
    "def translate_with_cohere(text, exclude_terms=None, custom_terms=None):\n",
    "    if exclude_terms is None:\n",
    "        exclude_terms = []\n",
    "    if custom_terms is None:\n",
    "        custom_terms = {}\n",
    " \n",
    "    exclusion_text = \"\\n\".join([f\"- {term}\" for term in exclude_terms])\n",
    "    custom_terms_text = \"\\n\".join([f\"'{eng_term}' => '{myanmar_term}'\" for eng_term, myanmar_term in custom_terms.items()])\n",
    "\n",
    " \n",
    "    prompt = f\"\"\"\n",
    "    Translate the following English text to Burmese (Myanmar) while keeping the original English terms for special terms.\n",
    "    Please exclude the following terms from translation: \n",
    "    {exclusion_text}\n",
    "    Also, translate the following custom terms as specified:\n",
    "    {custom_terms_text}\n",
    "\n",
    "    English text:\n",
    "    {text}\n",
    "    \"\"\"\n",
    "\n",
    "   \n",
    "    response = co.generate(\n",
    "        model=\"command-r-08-2024\", \n",
    "        prompt=prompt,\n",
    "        max_tokens=300,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "   \n",
    "    translated_text = response.generations[0].text\n",
    "\n",
    "    return translated_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_url = \"https://www.youtube.com/watch?v=DmgGGUYn2c8\"\n",
    "output_filename = \"trnascript.txt\"\n",
    "audio_file = download_audio(youtube_url)\n",
    "\n",
    "if audio_file:\n",
    "\n",
    "    transcript = transcribe_audio(audio_file)\n",
    "    \n",
    "    if transcript:\n",
    "\n",
    "        save_transcript_with_timestamps(transcript, output_filename)\n",
    "\n",
    "    os.remove(audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terms to Exclude: ['Damage', 'Ultimates / Liberation', 'Counters', 'DPS', 'Ult', 'master', 'Perfect', 'Stagger', 'Normal, Heavy, and Plunge Attacks', 'Outro', 'Gadget', 'Intro Skill', 'Stamina', 'Echoes', 'Heavy', 'the Utilities Menu', 'don’t', 'Skills', 'Gadgets', 'Concerto', 'Ultimate', 'Energy', 'Menu', 'Normal', 'The Forte Circuit', 'Sprinting', 'Bar', 'Plunge', 'Every Forte Circuit', 'Parries', 'Echo', 'Quick', 'Intro', 'Bars', 'Skill', 'Closing', 'the Forte Circuit', 'Swap', 'one', 'Circuit', 'Attacks', 'Concerto Energy', 'Forte', 'Utilities', 'Liberation', 'Ultimates', 'Forte Circuit', 'Dodge', 'Havoc', 'the Concerto Energy bar', 'Grapple', 'Thoughts', 'Counter', 'Dodge & Counters', 'Concerto Energy / Outro', 'their Intro Skill', 'One', 'Abilities', 'character', 'icon', 'players', 'player', 'play', 'damage', 'normal attack', 'action', 'actions']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "transcript_filepath = \"org.txt\"\n",
    "\n",
    "english_text = read_file(transcript_filepath)\n",
    "\n",
    "\n",
    "my_exclude_terms = [\"Abilities\", \"character\", \"icon\", \"players\", \"player\", \"play\", \"damage\", \"normal attack\", \"action\", \"actions\"]\n",
    "\n",
    "exclude_terms = detect_terms_to_exclude(english_text)\n",
    "exclude_terms = exclude_terms + my_exclude_terms\n",
    "print(\"Terms to Exclude:\", exclude_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_terms = {\n",
    "        \"for each character\": \"character တစ်ကောင်ချင်းစီအတွက်\",\n",
    "    }\n",
    "\n",
    "transcript_filepath = \"transcripts/transcript.txt\"\n",
    "\n",
    "original_timestamp_text = read_file(transcript_filepath)\n",
    "myanmar_text = generate_translation_prompt(original_timestamp_text, exclude_terms=exclude_terms, custom_terms=custom_terms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "save_file(\"prompt.txt\", myanmar_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"\\[\\d+\\.\\d{2} - \\d+\\.\\d{2}\\]\"\n",
    " \n",
    " \n",
    "text = read_file(\"transcripts/transcript.txt\") \n",
    "timestamps = re.findall(pattern, text)\n",
    "\n",
    "time_stamps =\"\" \n",
    "time_stamps += \"\\n\".join(timestamps)\n",
    "\n",
    "\n",
    "\n",
    "save_file(\"timestamps.txt\", time_stamps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'TTS'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msoundfile\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msf\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mTTS\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TTS\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load the pre-trained model\u001b[39;00m\n\u001b[0;32m      7\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtts_models/en/ljspeech/tacotron2-DDC\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'TTS'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    " \n",
    "\n",
    "# Load the pre-trained model\n",
    "model_name = \"tts_models/en/ljspeech/tacotron2-DDC\"\n",
    "tts = TTS(model_name)\n",
    "\n",
    "sample_rate = 22050  # Adjust the sample rate according to the model\n",
    "\n",
    "myanmar_text = \"\"\"\n",
    "ကဲ ဒါဆိုရင်တော့, character ရဲ့ Abilities တွေကို စတင်ကြည့်ရှုရအောင်\n",
    "Ult, Echo, Skill, Gadget, Forte Circuit, နဲ့ Concerto Energy.\n",
    "ဒါတွေက character တစ်ကောင်ချင်းစီအတွက်\n",
    "အဓိက Abilities တွေဖြစ်ပါတယ်။\n",
    "ပုံကဝိုင်း icon လေးဟာ  lock-on feature ဖြစ်ပါတယ်။\n",
    "\"\"\"\n",
    "\n",
    "def generate_audio(text):\n",
    "    # Generate audio using the TTS model\n",
    "    audio_data = tts.tts(text)\n",
    "    return audio_data\n",
    "\n",
    "# Extract English words and Myanmar text segments\n",
    "segments = re.split(r'(\\b[A-Za-z]+\\b)', myanmar_text)\n",
    "audio_segments = []\n",
    "\n",
    "for segment in segments:\n",
    "    segment = segment.strip()\n",
    "    if not segment:\n",
    "        continue\n",
    "    # Generate audio for each segment\n",
    "    print(f\"Generating audio for segment: {segment}\")\n",
    "    audio_segments.append(generate_audio(segment))\n",
    "\n",
    "# Combine all audio segments\n",
    "combined_audio = np.concatenate(audio_segments)\n",
    "\n",
    "# Save the combined audio to a file\n",
    "output_file = \"combined_audio.wav\"\n",
    "sf.write(output_file, combined_audio, sample_rate)\n",
    "print(f\"Audio saved as {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "c:\\Users\\heinm\\OneDrive\\Desktop\\transcript\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\heinm\\.cache\\huggingface\\hub\\models--hexgrad--Kokoro-82M. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unrecognized model in hexgrad/Kokoro-82M. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deformable_detr, deit, depth_anything, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, git, glm, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, graphormer, grounding-dino, groupvit, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rwkv, sam, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, siglip, siglip_vision_model, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, textnet, time_series_transformer, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zoedepth",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m english_model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhexgrad/Kokoro-82M\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     12\u001b[0m myanmar_model \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-to-speech\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmyanmar_model_name)\n\u001b[1;32m---> 13\u001b[0m english_model \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-to-speech\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menglish_model_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m sample_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16000\u001b[39m\n\u001b[0;32m     16\u001b[0m pause_duration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m  \n",
      "File \u001b[1;32mc:\\Users\\heinm\\OneDrive\\Desktop\\transcript\\.venv\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:849\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    846\u001b[0m                 adapter_config \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m    847\u001b[0m                 model \u001b[38;5;241m=\u001b[39m adapter_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase_model_name_or_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 849\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    852\u001b[0m     hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39m_commit_hash\n\u001b[0;32m    854\u001b[0m custom_tasks \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\heinm\\OneDrive\\Desktop\\transcript\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:1091\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m   1088\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(pretrained_model_name_or_path):\n\u001b[0;32m   1089\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m CONFIG_MAPPING[pattern]\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[1;32m-> 1091\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized model in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould have a `model_type` key in its \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, or contain one of the following strings \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min its name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(CONFIG_MAPPING\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1095\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Unrecognized model in hexgrad/Kokoro-82M. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deformable_detr, deit, depth_anything, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, git, glm, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, graphormer, grounding-dino, groupvit, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rwkv, sam, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, siglip, siglip_vision_model, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, textnet, time_series_transformer, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zoedepth"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import pipeline\n",
    "import soundfile as sf\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "\n",
    "myanmar_model_name = \"facebook/mms-tts-mya\"\n",
    "english_model_name = \"hexgrad/Kokoro-82M\"\n",
    "myanmar_model = pipeline(\"text-to-speech\", model=myanmar_model_name)\n",
    "english_model = pipeline(\"text-to-speech\", model=english_model_name)\n",
    "\n",
    "sample_rate = 16000\n",
    "pause_duration = 0.5  \n",
    "\n",
    "myanmar_text = \"\"\"\n",
    "ကဲ ဒါဆိုရင်တော့, character ရဲ့ Abilities တွေကို စတင်ကြည့်ရှုရအောင်\n",
    "Ult, Echo, Skill, Gadget, Forte Circuit, နဲ့ Concerto Energy.\n",
    "ဒါတွေက character တစ်ကောင်ချင်းစီအတွက်\n",
    "အဓိက Abilities တွေဖြစ်ပါတယ်။\n",
    "ပုံကဝိုင်း icon လေးဟာ  lock-on feature ဖြစ်ပါတယ်။\n",
    "\"\"\"\n",
    "\n",
    "def generate_audio(text, model):\n",
    "    speech = model(text)\n",
    "    audio_data = speech[\"audio\"]\n",
    "    if audio_data.ndim == 2 and audio_data.shape[0] == 1:\n",
    "        audio_data = audio_data.squeeze(0)\n",
    "    return audio_data\n",
    "\n",
    "\n",
    "# Extract English words and Myanmar text segments\n",
    "segments = re.split(r'(\\b[A-Za-z]+\\b)', myanmar_text)\n",
    "audio_segments = []\n",
    "\n",
    "for segment in segments:\n",
    "    segment = segment.strip()\n",
    "    if not segment:\n",
    "        continue\n",
    "    if re.match(r'^[A-Za-z]+$', segment):\n",
    "        # Generate audio for English words\n",
    "        print(f\"segment (English): {segment}\")\n",
    "        audio_segments.append(generate_audio(segment, english_model))\n",
    "    else:\n",
    "        if segment.strip() in [\",\", \".\", \"။\", \"၊\", \":\", \";\",\"-\"]:\n",
    "            continue\n",
    "        # Generate audio for Myanmar text\n",
    "        print(f\"transcript (Myanmar): {segment}\")\n",
    "        audio_segments.append(generate_audio(segment, myanmar_model))\n",
    "    \n",
    "    # Add pause for newline characters\n",
    "    if '\\n' in segment:\n",
    "        pause = np.zeros(int(pause_duration * sample_rate)) \n",
    "        audio_segments.append(pause)\n",
    "\n",
    "# Combine all audio segments\n",
    "combined_audio = np.concatenate(audio_segments)\n",
    " \n",
    "output_file = \"combined_audio.wav\"\n",
    "sf.write(output_file, combined_audio, sample_rate)\n",
    "print(f\"Audio saved as {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'git' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 2] The system cannot find the file specified: 'Kokoro-82M'\n",
      "c:\\Users\\heinm\\OneDrive\\Desktop\\transcript\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'git' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "The system cannot find the path specified.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install -q phonemizer torch transformers scipy munch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 2️⃣ Build the model and load the default voicepack\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m build_model\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     10\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "import soundfile as sf\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the pre-trained models\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "myanmar_model_name = \"facebook/mms-tts-mya\"\n",
    "myanmar_model = pipeline(\"text-to-speech\", model=myanmar_model_name, device=device)\n",
    "\n",
    "# Load Kokoro-TTS model\n",
    "from models import build_model\n",
    "MODEL = build_model('kokoro-v0_19.pth', device)\n",
    "VOICE_NAME = [\n",
    "    'af', # Default voice is a 50-50 mix of Bella & Sarah\n",
    "    'af_bella', 'af_sarah', 'am_adam', 'am_michael',\n",
    "    'bf_emma', 'bf_isabella', 'bm_george', 'bm_lewis',\n",
    "    'af_nicole', 'af_sky',\n",
    "][0]\n",
    "VOICEPACK = torch.load(f'voices/{VOICE_NAME}.pt', weights_only=True).to(device)\n",
    "print(f'Loaded voice: {VOICE_NAME}')\n",
    "\n",
    "# Sample rate for the output audio\n",
    "sample_rate = 16000\n",
    "pause_duration = 0.5  \n",
    "\n",
    "myanmar_text = \"\"\"\n",
    "ကဲ ဒါဆိုရင်တော့, character ရဲ့ Abilities တွေကို စတင်ကြည့်ရှုရအောင်\n",
    "Ult, Echo, Skill, Gadget, Forte Circuit, နဲ့ Concerto Energy.\n",
    "ဒါတွေက character တစ်ကောင်ချင်းစီအတွက်\n",
    "အဓိက Abilities တွေဖြစ်ပါတယ်။\n",
    "ပုံကဝိုင်း icon လေးဟာ  lock-on feature ဖြစ်ပါတယ်။\n",
    "\"\"\"\n",
    "\n",
    "def generate_audio(text, model, voicepack=None, lang=None):\n",
    "    if model == \"kokoro\":\n",
    "        from kokoro import generate\n",
    "        audio, _ = generate(MODEL, text, voicepack, lang=lang)\n",
    "        return audio\n",
    "    else:\n",
    "        speech = model(text)\n",
    "        audio_data = speech[\"audio\"]\n",
    "        if audio_data.ndim == 2 and audio_data.shape[0] == 1:\n",
    "            audio_data = audio_data.squeeze(0)\n",
    "        return audio_data\n",
    "\n",
    "# Extract English words and Myanmar text segments\n",
    "segments = re.split(r'(\\b[A-Za-z]+\\b)', myanmar_text)\n",
    "audio_segments = []\n",
    "\n",
    "for segment in segments:\n",
    "    segment = segment.strip()\n",
    "    if not segment:\n",
    "        continue\n",
    "    if re.match(r'^[A-Za-z]+$', segment):\n",
    "        # Generate audio for English words using Kokoro-TTS\n",
    "        print(f\"segment (English): {segment}\")\n",
    "        audio_segments.append(generate_audio(segment, \"kokoro\", VOICEPACK, lang=VOICE_NAME[0]))\n",
    "    else:\n",
    "        if segment.strip() in [\",\", \".\", \"။\", \"၊\", \":\", \";\",\"-\"]:\n",
    "            continue\n",
    "        # Generate audio for Myanmar text using MMS-TTS\n",
    "        print(f\"transcript (Myanmar): {segment}\")\n",
    "        audio_segments.append(generate_audio(segment, myanmar_model))\n",
    "    \n",
    "    # Add pause for newline characters\n",
    "    if '\\n' in segment:\n",
    "        pause = np.zeros(int(pause_duration * sample_rate)) \n",
    "        audio_segments.append(pause)\n",
    "\n",
    "# Combine all audio segments\n",
    "combined_audio = np.concatenate(audio_segments)\n",
    "\n",
    "# Save the combined audio to a file\n",
    "output_file = \"combined_audio.wav\"\n",
    "sf.write(output_file, combined_audio, sample_rate)\n",
    "print(f\"Audio saved as {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
